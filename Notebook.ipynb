{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Depot Product Search Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search relevancy is an implicit measure Home Depot uses to gauge how quickly they can get customers to the right products. This script focuses on predicting accurate Search relevancy of every search query in homedepot's search relelvance dataset on Kaggle(https://www.kaggle.com/c/home-depot-product-search-relevance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import gensim\n",
    "from porter2stemmer import Porter2Stemmer\n",
    "from sklearn.metrics import r2_score\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Helper Function(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fucntion to compute number of common terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_count(str):\n",
    "    stra, strb = str.split('\\t')\n",
    "    count = 0\n",
    "    for word in stra.strip().split(' '):\n",
    "        if strb.find(word) >= 0:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Float function to compute similarity in embedded meaning of words (Using Word2Vec network embeddings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wv_sim(str, model):\n",
    "    stra, strb = str.split('\\t')\n",
    "    count = 0.0\n",
    "    wc = 0\n",
    "    for word in stra.strip().split(' '):\n",
    "        if word in model.wv.vocab:\n",
    "            agg = 0\n",
    "            for term in strb.strip().split(' '):\n",
    "                if term in model.wv.vocab:\n",
    "                    tx = (model.wv.similarity(word, term))\n",
    "                    if(tx > agg):\n",
    "                        agg = tx\n",
    "            count += agg\n",
    "            wc += 1\n",
    "    return count / (wc if wc > 0 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levenshtein Distance functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_set(x):\n",
    "    stra, strb = x.split('\\t')\n",
    "    return fuzz.token_set_ratio(stra, strb)\n",
    "\n",
    "def token_sort(x):\n",
    "    stra, strb = x.split('\\t')\n",
    "    return fuzz.token_sort_ratio(stra, strb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a number of products and real customer search terms from Home Depot's website. Most important files are: train.csv, test.csv, product_descriptions.csv and attributes.csv.\n",
    "\n",
    "Training data consists of 74067 instances and Test data contains 166693 instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = dict()\n",
    "dfs['train'] = pd.read_csv('train.csv', encoding = \"ISO-8859-1\")\n",
    "dfs['test'] = pd.read_csv('test.csv', encoding = \"ISO-8859-1\")\n",
    "dsc = pd.read_csv('product_descriptions.csv', encoding = \"utf-8\")\n",
    "att = pd.read_csv('attributes.csv', encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Brand Information and additional description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = Porter2Stemmer()\n",
    "aux_dsc = att[att['name'] == 'Bullet01'][[\"product_uid\", \"value\"]].rename(columns = {\"value\" : \"auxilary_description\"})\n",
    "brands_list = att[att['name'] == 'MFG Brand Name'][[\"product_uid\", \"value\"]].rename(columns = {\"value\" : \"brand\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract unique sentences from dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array([])\n",
    "for i, k in enumerate(dfs):\n",
    "    for field in ['product_title', 'search_term']:\n",
    "        sentences = np.append(sentences, dfs[k][field].map(lambda x: [stemmer.stem(word) for word in str(x).lower().split(' ')]).values)\n",
    "sentences = np.append(sentences, dsc['product_description'].map(lambda x: [stemmer.stem(word) for word in str(x).lower().split(' ')]).values)\n",
    "sentences = np.append(sentences, brands_list['brand'].map(lambda x: [stemmer.stem(word) for word in str(x).lower().split(' ')]).values)\n",
    "sentences = np.append(sentences, aux_dsc['auxilary_description'].map(lambda x: [stemmer.stem(word) for word in str(x).lower().split(' ')]).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Word2Vec Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = Word2Vec(sentences, size=16, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stem dataset, Calculate query length and similarity coefficients between search query & different product fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, k in enumerate(dfs):\n",
    "    dfs[k] = pd.merge(dfs[k], dsc, how='left', on='product_uid')\n",
    "    dfs[k] = pd.merge(dfs[k], brands_list, how='left', on='product_uid')\n",
    "    dfs[k] = pd.merge(dfs[k], aux_dsc, how='left', on='product_uid')\n",
    "    \n",
    "    for field in ['product_title', 'brand', 'product_description', 'auxilary_description', 'search_term']:\n",
    "        dfs[k][field] = dfs[k][field].map(lambda x: \" \".join([stemmer.stem(word) for word in str(x).lower().split(' ')]))\n",
    "    dfs[k]['search_len'] = dfs[k]['search_term'].map(lambda x: len(x.split(' '))).astype(np.int32)\n",
    "    \n",
    "    for field in ['product_title', 'brand', 'product_description', 'auxilary_description']:\n",
    "        dfs[k][field + '_common_count'] = (dfs[k]['search_term'] + '\\t' + dfs[k][field]).map(lambda x: common_count(x)).astype(np.float32)\n",
    "        dfs[k][field + '_wv_sim'] = (dfs[k]['search_term'] + '\\t' + dfs[k][field]).map(lambda x: wv_sim(x, wv_model)).astype(np.float32)\n",
    "        dfs[k][field + '_token_set'] = (dfs[k]['search_term'] + '\\t' + dfs[k][field]).map(lambda x: token_set(x)).astype(np.float32)\n",
    "        dfs[k][field + '_token_sort'] = (dfs[k]['search_term'] + '\\t' + dfs[k][field]).map(lambda x: token_sort(x)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = dfs['train']['relevance'].values\n",
    "x_test = dfs['test'][['search_len', 'product_title_common_count', 'product_title_wv_sim', 'product_title_token_set', 'product_title_token_sort', 'brand_common_count', 'brand_wv_sim', 'brand_token_set', 'brand_token_sort', 'product_description_common_count','product_description_wv_sim', 'product_description_token_set', 'product_description_token_sort', 'auxilary_description_common_count', 'auxilary_description_wv_sim', 'auxilary_description_token_set', 'auxilary_description_token_sort']].values\n",
    "x_train = dfs['train'][['search_len', 'product_title_common_count', 'product_title_wv_sim', 'product_title_token_set', 'product_title_token_sort', 'brand_common_count', 'brand_wv_sim', 'brand_token_set', 'brand_token_sort', 'product_description_common_count','product_description_wv_sim', 'product_description_token_set', 'product_description_token_sort', 'auxilary_description_common_count', 'auxilary_description_wv_sim', 'auxilary_description_token_set', 'auxilary_description_token_sort']].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.08, max_delta_step=0, max_depth=7,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=0.75)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75, colsample_bytree=1, max_depth=7)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.308294417585\n"
     ]
    }
   ],
   "source": [
    "y = model.predict(x_train)\n",
    "print(r2_score(y_train, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMS Error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4441052537155975"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((y_train - y) ** 2) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = model.predict(x_test)\n",
    "ans = pd.DataFrame({\"id\": dfs['test']['id'], \"relevance\": y})\n",
    "ans.to_csv('answers.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XG-Boost Regressor yields average mean squared error of 0.47179 on Kaggle (vs sklearn's Random Forest score of 0.48221 and Keras' ANN score of 0.51127)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
