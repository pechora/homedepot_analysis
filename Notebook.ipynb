{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Depot Product Search Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search relevancy is an implicit measure Home Depot uses to gauge how quickly they can get customers to the right products. This script focuses on predicting accurate Search relevancy of every search query in homedepot's search relelvance dataset on Kaggle(https://www.kaggle.com/c/home-depot-product-search-relevance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from porter2stemmer import Porter2Stemmer\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Helper Function(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fucntion to compute number of common terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_count(str):\n",
    "    stra, strb = str.split('\\t')\n",
    "    count = 0\n",
    "    for word in stra.strip().split(' '):\n",
    "        if strb.find(word) >= 0:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a number of products and real customer search terms from Home Depot's website. Most important files are: train.csv, test.csv, product_descriptions.csv and attributes.csv.\n",
    "\n",
    "Training data consists of 74067 instances and Test data contains 166693 instances. In this script, we'll use number of common terms between the search query and various product properties/descriptions to analyze the relevance of a giver query-product pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = dict()\n",
    "dfs['train'] = pd.read_csv('train.csv', encoding = \"ISO-8859-1\")\n",
    "dfs['test'] = pd.read_csv('test.csv', encoding = \"ISO-8859-1\")\n",
    "dsc = pd.read_csv('product_descriptions.csv', encoding = \"utf-8\")\n",
    "att = pd.read_csv('attributes.csv', encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Brand Information and additional description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = Porter2Stemmer()\n",
    "aux_dsc = att[att['name'] == 'Bullet01'][[\"product_uid\", \"value\"]].rename(columns = {\"value\" : \"auxilary_description\"})\n",
    "brands_list = att[att['name'] == 'MFG Brand Name'][[\"product_uid\", \"value\"]].rename(columns = {\"value\" : \"brand\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stem dataset, Calculate query length and common terms between search query & different product fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, k in enumerate(dfs):\n",
    "    dfs[k] = pd.merge(dfs[k], dsc, how = 'left', on='product_uid')\n",
    "    dfs[k] = pd.merge(dfs[k], aux_dsc, how = 'left', on='product_uid')\n",
    "    dfs[k] = pd.merge(dfs[k], brands_list, how = 'left', on='product_uid')\n",
    "\n",
    "    for field in ['product_title', 'brand', 'product_description', 'auxilary_description', 'search_term']:\n",
    "        dfs[k][field] = dfs[k][field].map(lambda x: \" \".join([stemmer.stem(word) for word in str(x).lower().split(' ')]))\n",
    "\n",
    "    dfs[k]['search_len'] = dfs[k]['search_term'].map(lambda x: len(x.split(' '))).astype(np.int32)\n",
    "    dfs[k]['title_common_count'] = (dfs[k]['search_term'] + '\\t' + dfs[k]['product_title']).map(lambda x: common_count(x)).astype(np.int32)\n",
    "    dfs[k]['brand_common_count'] = (dfs[k]['search_term'] + '\\t' + dfs[k]['brand']).map(lambda x: common_count(x)).astype(np.int32)\n",
    "    dfs[k]['desc_common_count'] = (dfs[k]['search_term'] + '\\t' + dfs[k]['product_description']).map(lambda x: common_count(x)).astype(np.int32)\n",
    "    dfs[k]['aux_desc_common_count'] = (dfs[k]['search_term'] + '\\t' + dfs[k]['auxilary_description']).map(lambda x: common_count(x)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = dfs['train'][['search_len', 'title_common_count', 'brand_common_count', 'desc_common_count', 'aux_desc_common_count']].values\n",
    "y_train = dfs['train']['relevance'].values\n",
    "x_test = dfs['test'][['search_len', 'title_common_count', 'brand_common_count', 'desc_common_count', 'aux_desc_common_count']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(n_estimators=60, learning_rate=0.08, gamma=0, subsample=0.75, colsample_bytree=1, max_depth=7)\n",
    "model.fit(x_train, y_train)\n",
    "y = model.predict(x_test)\n",
    "ans = pd.DataFrame({\"id\": dfs['test']['id'], \"relevance\": y})\n",
    "ans.to_csv('answers.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XG-Boost Regressor yields average mean squared error of 0.48403 on Kaggle(vs sklearn's Random Forest score of 0.49176 and Keras' ANN score of 0.50260)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
